\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}

%\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage[francais, english]{babel} 
\usepackage{amsmath, amsthm}
\usepackage{xcolor}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\usepackage{tikz}
\usetikzlibrary{automata, positioning}

%\usepackage{multicol} (Stationary distribution)
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem*{proof*}{Proof}
\newtheorem{definition}{Definition}
\usepackage{graphicx}

\newtheorem{exercise}{Exercise}
\newtheorem{question}{Question}

\newtheorem{example}{Example}

\newtheorem{remark}{Remark}

\usepackage[a4paper,left=1.5cm, right=1.5cm,top=1.2cm,bottom=1.4cm]{geometry}
\usepackage{cancel}
\usepackage{bm}
\usepackage{amssymb,amsfonts}
\usepackage{mathrsfs}
\usepackage{color}
%\usepackage{hyperref}
\usepackage{dsfont}
\usepackage{algorithmicx}
\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{marginnote}
\newcommand{\Ptr}{\mathcal P^{\rm tr}}
\newcommand{\tr}{{\rm tr}}
\newcommand{\N}{\mathbb N}
\newcommand{\calN}{\mathcal N}
\newcommand{\bP}{\bold P}
\newcommand{\calK}{\mathcal K}
\newcommand{\calF}{\mathcal F}
\newcommand{\calH}{\mathcal H}
\newcommand{\calS}{\mathcal S}

\newcommand{\bQ}{\bold Q}
\newcommand{\bA}{\bold A}
\newcommand{\bD}{\bold D}
\newcommand{\bI}{\bold I}
\newcommand{\bG}{\bold G}
\newcommand{\bs}{\bold s}
\newcommand{\calP}{\mathcal P}
\newcommand{\calC}{\mathcal C}
\newcommand\red[1]{\textcolor{red} {#1} }
\newcommand{\R}{\mathbb R}
\newcommand{\bX}{\bar X}
\newcommand{\Ktr}{\calK^{\rm tr}}
\title{\bfseries \Huge Lab session: Markov Chain Monte Carlo}
\author{}
\date{\today}

\begin{document}
	\maketitle
	\section{Introduction}
	
	So far in DTMCs, we were given a Markov chain $(X_n)_{n\in \N}$ on a finite state space $\calS$ and we then studied the existence and uniqueness of its stationary distribution and convergence to it. 
	In this lab, we will consider the reverse problem :
	
	\textit{Given a probability distribution $\bm\pi$ on a sample space $\calS$, can we construct a Markov chain $(X_t)_{t \geq 0}$ such that $\bm\pi$ is a stationary distribution?}
	
	If, in addition, the chain is irreducible and aperiodic, then by the convergence theorem, we know that the distribution $\bm\pi_t$ of $X_t$ converges to $\bm\pi$. Hence, if we run the chain for long enough, the state of the chain is asymptotically distributed as $\bm\pi$. In other words, we can sample a random element of $\calS$ according to the prescribed distribution $\bm\pi$ by emulating it through a suitable Markov chain. This method of sampling is called Markov chain Monte Carlo (MCMC).
	
	The algorithm  we focus on is the well-known \textbf{Metropolis-Hastings algorithm} for MCMC sampling.
	
	\section{The Metropolis-Hastings algorithm}
	Let $\bP$ be a transition matrix of a Markov chain on a finite state space $\calS = \{1, 2, \dots, m\}$, $m\in\N^*$. 
	Let $\bm\pi$ be a probability distribution on $\calS$, which is not necessarily a stationary distribution for $\bP$. Our goal is to design a Markov chain on $\calS$ that has $\bm\pi$ as its stationary distribution.
	
	The idea is to construct a new Markov chain that uses $\bP$ to propose transitions, and then decides whether to accept or reject these proposals using a suitable acceptance probability.
	
	Fix an $m \times m$ matrix $\bm A$ with entries from $[0, 1]$. Consider a Markov chain $(X_t)_{t \geq 0}$ on $\calS$ defined as follows :
	\begin{itemize}
	\item \textbf{Generation Step :} Suppose the current state is $X_t = a\in\calS$. Generate a candidate state $b \in \calS$ according to the proposal distribution $\bP(a, \cdot)$, where $\bm P(a, \cdot)$ is the $a$-th row vector of $\bP$.
	
	\item \textbf{Rejection Step :} Flip an independent coin with success probability $\bm A(a, b)$, where $\bm A(a, b)$ is the value of $\bm A$ at its $a$-th row and $b$-th column :
	\begin{itemize}
		\item If the coin flip is successful (i.e., with probability $\bm A(a, b)$), accept the proposed move and set $X_{t+1} = b$.
		\item Otherwise (with probability $1 - \bm A(a, b)$), reject the move and remain at the current state: $X_{t+1} = a$.
	\end{itemize}
\end{itemize}
	
	Here, the entry $\bm A(a, b)$ is called the \textbf{acceptance probability} of the move from $a$ to $b$.
	
	\begin{enumerate}
		\item Let \( \bQ \) denote the matrix of the chain $(X_n)_{n\in\N}$ described above. Show that :
		\[
		\bQ(a, b) = 
		\begin{cases}
			\bP(a, b) \bA(a, b) & \text{if } b \neq a, \\
			1 - \underset{c\in\calS, c \neq a}{\sum} \bP(a, c) \bA(a, c) & \text{if } b = a.
		\end{cases}
		\]
		
		\item Show that if \( \bm\pi(x) \bQ(x, y) = \bm\pi(y) \bQ(y, x) \) for all \( x, y \in \calS \) such that \( x \neq y \), then  \( \bm\pi \bQ = \bm\pi \). 
		
		\item Deduce that if
		\[
		\bm\pi(x) \bP(x, y) \bA(x, y) = \bm\pi(y) \bP(y, x) \bA(y, x)
		,\quad \forall x, y \in \calS, x\neq y,
		\]
		then \( \bm\pi  \) is a stationary distribution for \( (X_n)_{n\in\N} \).
		
		\item We are also interested in fast convergence of the Markov chain. Thus, we want to choose the acceptance probability \( \bA(a, b) \in [0, 1] \) as large as possible for each \( a, b \in \calS \). Show that the following choice
		\[
		\bA(x, y) = \min \left( \frac{\bm\pi(y) \bP(y, x)}{\bm\pi(x) \bP(x, y)}, 1 \right)
		\]
		for all \( x, y \in \calS \), \( x \neq y \), satisfies the condition in the previous question, and each \( \bA(x, y) \) is maximized for all \( x \neq y \).
		
		(iv) Let \( (Y_t)_{t \geq 0} \) be a random walk on the 5-wheel graph \( G = (V, E) \) as shown in Figure 8. Show that
		\[
		\pi = \left( \frac{6}{20}, \frac{5}{20}, \frac{2}{20}, \frac{1}{20}, \frac{4}{20}, \frac{3}{20} \right)
		\]
		is the unique stationary distribution of \( Y_t \). Apply the Metropolis-Hastings algorithm derived in (i)-(iii) above to modify \( Y_t \) to obtain a new Markov chain \( X_t \) on \( V \) that converges to \( \text{Uniform}(V) \) in distribution.
		
	\end{enumerate}
	
	
	We claim that if
	\[
	\bm\pi(x) Q(x, y) = \bm\pi(y) Q(y, x) \quad \forall x, y \in \calS,\, x \neq y
	\]
	then $\bm\pi Q = \bm\pi$, i.e., $\bm\pi$ is a stationary distribution of the chain.
	
	Now, suppose that
	\[
	\bm\pi(x) P(x, y) A(x, y) = \bm\pi(y) P(y, x) A(y, x) \quad \forall x, y \in \calS,\ x \neq y
	\]
	then the detailed balance condition holds for $Q$, hence $\bm\pi$ is a stationary distribution for $(X_t)_{t \geq 0}$.
	
	\section*{(iii) Choice of Acceptance Probability}
	
	Since we want the Markov chain to converge quickly, we choose the acceptance probability $A(a, b) \in [0, 1]$ to be as large as possible for all $a, b \in \calS$. The following choice:
	\[
	A(x, y) = \min\left(1, \frac{\bm\pi(y) P(y, x)}{\bm\pi(x) P(x, y)}\right) = \frac{\bm\pi(y) P(y, x)}{\bm\pi(x) P(x, y)} \wedge 1
	\quad \forall x, y \in \calS,\ x \neq y
	\]
	satisfies the condition in (ii) and ensures that each $A(x, y)$ is maximized under the constraint of symmetry in detailed balance.
	
	\section*{(iv) Application: Random Walk on the 5-Wheel Graph}
	
	Let $(Y_t)_{t \geq 0}$ be a random walk on the 5-wheel graph $G = (V, E)$ as shown in Figure 8. The state space is $V = \{1, 2, 3, 4, 5, 6\}$.
	
	The unique stationary distribution of $(Y_t)$ is:
	\[
	\bm\pi = \left[ \frac{3}{20}, \frac{3}{20}, \frac{3}{20}, \frac{3}{20}, \frac{3}{20}, \frac{5}{20} \right]
	\]
	
	We apply the Metropolis-Hastings algorithm derived above to modify $(Y_t)$ into a new Markov chain $(X_t)$ on $V$ such that the stationary distribution is uniform on $V$, i.e.,
	\[
	\bm\pi_{\text{target}} = \left[ \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6}, \frac{1}{6} \right]
	\]
	
	Let $P(x, y)$ be the transition probabilities of $(Y_t)$. Define acceptance probabilities:
	\[
	A(x, y) = \min\left(1, \frac{\bm\pi_{\text{target}}(y) P(y, x)}{\bm\pi_{\text{target}}(x) P(x, y)}\right)
	\]
	Then, define the transition matrix $Q$ for $(X_t)$ using the formula in part (i). The resulting chain will converge in distribution to $\bm\pi_{\text{target}} = \text{Uniform}(V)$.
	
	
	
	
	
	\section*{Learning Objectives}
	By the end of this lab, you should be able to :
	\begin{itemize}
		\item Understand the relevance of Markov chains in sampling.
		\item Implement the Metropolis-Hastings algorithm in Python.
		\item Analyze and interpret the behavior of MCMC samplers.
		\item Apply the algorithm to both continuous and discrete probability distributions.
	\end{itemize}
	
	\section*{Part 1: Conceptual Questions}
	
	\begin{enumerate}[label=\textbf{Q\arabic*.}]
		\item Define a Markov chain and explain what is meant by the stationary distribution.
		
		\item Why do we use MCMC methods in Bayesian inference? What problems do they help to solve?
		
		\item Describe each step of the Metropolis-Hastings algorithm. What role does the proposal distribution play?
		
		\item What could happen if the proposal distributionâ€™s variance is too large or too small?
		
		\item How would you evaluate if your Markov chain has converged?
		
		\item For the Gaussian example:
		\begin{itemize}
			\item Plot the histogram of samples and overlay the target PDF.
			\item What does the trace plot reveal about the sampler?
			\item How does changing the proposal standard deviation affect the sampling?
		\end{itemize}
		
		\item For the Zipf distribution:
		\begin{itemize}
			\item Compare the sampled histogram with the true PMF.
			\item What difficulties arise when sampling from heavy-tailed or discrete distributions?
		\end{itemize}
	\end{enumerate}
	
	\section*{Extension Questions (Optional)}
	
	\begin{enumerate}[label=\textbf{E\arabic*.}]
		\item Modify the sampler to work on a 2D target distribution (e.g., a bivariate Gaussian). What changes are necessary?
		\item Implement a different discrete proposal strategy for the Zipf sampler (e.g., geometric steps).
		\item Compare acceptance rates across different samplers. What rate seems to balance convergence and exploration?
	\end{enumerate}
	
	\paragraph{References :} MATH 171 Stochastic Processes Lecture Notes (Hanbaek Lyu).
\end{document}
